# ### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)
import numpy as np
import pandas as pd
import evaluate
from transformers import (
    GenerationConfig,
)

# custom imports
from generator.prompt import zero_prompt
from generator.utility import get_logger

# Setup logger
log = get_logger()

dash_line = "=" * 50
rouge = evaluate.load("rouge")


def get_trainable_model_pars(model):
    trainable_model_params = 0
    all_model_params = 0
    for _, param in model.named_parameters():
        all_model_params += param.numel()
        if param.requires_grad:
            trainable_model_params += param.numel()
    return f"trainable model parameters: {trainable_model_params}\nall model parameters: {all_model_params}\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%"


def show_original_instruct_summary(
    dataset, tokenizer, original_model, instruct_model, index=200
):
    prompt = zero_prompt(dataset, index=index)
    human_baseline_summary = dataset["test"][index]["summary"]

    input_ids = tokenizer(prompt, return_tensors="pt").input_ids

    original_model_outputs = original_model.generate(
        input_ids=input_ids,
        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),
    )
    original_model_text_output = tokenizer.decode(
        original_model_outputs[0], skip_special_tokens=True
    )

    instruct_model_outputs = instruct_model.generate(
        input_ids=input_ids,
        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1),
    )
    instruct_model_text_output = tokenizer.decode(
        instruct_model_outputs[0], skip_special_tokens=True
    )

    log.info(dash_line)
    log.info(f"BASELINE PATCH:\n{human_baseline_summary}")
    log.info(dash_line)
    log.info(f"ORIGINAL MODEL:\n{original_model_text_output}")
    log.info(dash_line)
    log.info(f"INSTRUCT MODEL:\n{instruct_model_text_output}")


def evaluate_rouge(results):
    """ Evaluate the summaries generated by the models using the ROUGE metric """
    human_baseline_summaries = results["human_baseline_summaries"].values
    original_model_summaries = results["original_model_summaries"].values
    instruct_model_summaries = results["instruct_model_summaries"].values

    original_model_results = rouge.compute(
        predictions=original_model_summaries,
        references=human_baseline_summaries[0 : len(original_model_summaries)],
        use_aggregator=True,
        use_stemmer=True,
    )

    instruct_model_results = rouge.compute(
        predictions=instruct_model_summaries,
        references=human_baseline_summaries[0 : len(instruct_model_summaries)],
        use_aggregator=True,
        use_stemmer=True,
    )

    log.info("ORIGINAL MODEL:")
    log.info(original_model_results)
    log.info("INSTRUCT MODEL:")
    log.info(instruct_model_results)

    log.info("Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL")

    improvement = np.array(list(instruct_model_results.values())) - np.array(
        list(original_model_results.values())
    )
    for key, value in zip(instruct_model_results.keys(), improvement):
        log.info(f"{key}: {value*100:.2f}%")


def generate_summaries(
    original_model,
    instruct_model,
    tokenizer,
    dialogues,
    human_baseline_summaries,
    result_csv,
):
    """" Generate summaries for a list of dialogues using a model """
    original_model_summaries = []
    instruct_model_summaries = []
    for _, dialogue in enumerate(dialogues):
        prompt = f"""
                    Summarize the following conversation.

                    {dialogue}

                    Summary: """
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids

        original_model_outputs = original_model.generate(
            input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200)
        )
        original_model_text_output = tokenizer.decode(
            original_model_outputs[0], skip_special_tokens=True
        )

        original_model_summaries.append(original_model_text_output)

        instruct_model_outputs = instruct_model.generate(
            input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200)
        )
        instruct_model_text_output = tokenizer.decode(
            instruct_model_outputs[0], skip_special_tokens=True
        )

        instruct_model_summaries.append(instruct_model_text_output)

    zipped_summaries = list(
        zip(
            human_baseline_summaries, original_model_summaries, instruct_model_summaries
        )
    )

    df = pd.DataFrame(
        zipped_summaries,
        columns=[
            "human_baseline_summaries",
            "original_model_summaries",
            "instruct_model_summaries",
        ],
    )
    df.to_csv(result_csv, index=False)
    log.info(dash_line)
    log.info(f"Results of vul-fix-training saved to {result_csv}")
    log.info(dash_line)
    log.info("Sample of the results:")
    log.info(df.head())
    log.info(dash_line)
    return df
